{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codi TFG Mireia Almena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per a realitzar l'estudi s'han desenvolupat diverses funcions per avaluar i processar cada conjunt de dades. S'ha utilitzat Google Colab.\n",
    "\n",
    "Els models utilitzats són `Catalan BERTa-v2`, `M-MOD`, `TwHIN-BERT` i `XLM-V`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer que tot, s'han importat les llibreries utiltzades: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from google.colab import files\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer, FillMaskPipeline\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Després, s'han importat els conjunts de dades. Que es troben en la carpeta de datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded = files.upload() \n",
    "\n",
    "my_dataset = pd.read_csv(\"dataset.csv\")\n",
    "anafora_dataset = pd.read_csv(\"anafora_dataset.csv\")\n",
    "dom_dataset = pd.read_csv(\"dom_dataset.csv\")\n",
    "ser_estar_dataset = pd.read_csv(\"ser_estar_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitat lingüística\n",
    "Per analitzar la qualitat lingüística dels models mitjançant Masked Language Models, s’han implementat dues funcions principals. La primera detecta si la primera predicció i les cinc primeres prediccions generades pel model per a una frase amb el marcador `<mask>` corresponen a la resposta normativa o no normativa prèviament establerta, o bé si cap d’elles coincideix amb les respostes definides: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_modeling(models, dataset):\n",
    "  tokenizer_hf = AutoTokenizer.from_pretrained(models)\n",
    "  model = AutoModelForMaskedLM.from_pretrained(models)\n",
    "  if models == 'facebook/xmod-base':\n",
    "    model.set_default_language(\"ca_ES\")\n",
    "  pipeline = FillMaskPipeline(model, tokenizer_hf)\n",
    "\n",
    "#Per treure els valors buits \n",
    "  dataset = dataset.dropna() \n",
    "\n",
    "#Comptador per després\n",
    "#'Top 1' fa referència a la primera predicció, i 'Top 5' a les cinc primeres\n",
    "  normatives_top5 = 0\n",
    "  normatives_top1 = 0\n",
    "  no_normatives_top5= 0\n",
    "  no_normatives_top1 = 0\n",
    "  altres = 0\n",
    "\n",
    "  prediccions_columna = [] #per afegir la columna de prediccions al dataset\n",
    "\n",
    "#Posa en una variable totes les frases amb <masked>, les respostes normatives i les no normatives; i fa la predicció per a cada frase amb <masked>\n",
    "\n",
    "  for index, row in dataset.iterrows():\n",
    "    normatiu = row['Resposta normativa']\n",
    "    no_normatiu = row['Resposta no normativa freqüent']\n",
    "    masked = row[\"Masked\"]\n",
    "    res_hf = pipeline(masked)\n",
    "    prediccions =[r['token_str'] for r in res_hf]\n",
    "    prediccions_sense_espais = [pred.strip() for pred in prediccions] #fem strip perquè no surtin espais\n",
    "\n",
    "    if normatiu == prediccions_sense_espais[0]: #utilitzem l'índex [0] per trobar la primera predicció\n",
    "      normatives_top1 +=1\n",
    "    if normatiu in prediccions_sense_espais:\n",
    "        normatives_top5 +=1\n",
    "    if no_normatiu==prediccions_sense_espais[0]:\n",
    "      no_normatives_top1 +=1\n",
    "    if no_normatiu in prediccions_sense_espais:\n",
    "      no_normatives_top5 +=1\n",
    "    if normatiu not in prediccions_sense_espais and no_normatiu not in prediccions_sense_espais:\n",
    "       altres +=1\n",
    "\n",
    "    prediccions_columna.append(prediccions_sense_espais)\n",
    "\n",
    "\n",
    "#Afegim les noves columnes\n",
    "  dataset = dataset.copy() #sense això em sortien errors\n",
    "  dataset[\"Prediccions\"] = prediccions_columna\n",
    "  dataset[\"Model\"] = models\n",
    "\n",
    "\n",
    "  dataset[\"Normatives top 1\"] = None\n",
    "  dataset[\"Normatives top 5\"] = None\n",
    "  dataset[\"No normatives top 1\"] = None\n",
    "  dataset[\"No normatives top 5\"] = None\n",
    "  dataset[\"Altres\"] = None\n",
    "\n",
    "  # Assignem només a la primera fila\n",
    "  dataset.loc[dataset.index[0], \"Normatives top 1\"] = normatives_top1\n",
    "  dataset.loc[dataset.index[0], \"Normatives top 5\"] = normatives_top5\n",
    "  dataset.loc[dataset.index[0], \"No normatives top 1\"] = no_normatives_top1\n",
    "  dataset.loc[dataset.index[0], \"No normatives top 5\"] = no_normatives_top5\n",
    "  dataset.loc[dataset.index[0], \"Altres\"] = altres\n",
    "\n",
    "  nom_fitxer = \"resultats_qualitat.csv\"\n",
    "\n",
    "  fitxer_existeix = os.path.isfile(nom_fitxer) #mira si troba el fitxer als files\n",
    "  dataset.to_csv(nom_fitxer, mode='a', index=False, header=not fitxer_existeix) #si existeix fa append del nou contingut\n",
    "  files.download(nom_fitxer) #ho descarrega\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem una llista dels models per processar-los tots alhora: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llista_models = [\n",
    "    'projecte-aina/roberta-base-ca-v2',\n",
    "    'facebook/xmod-base',\n",
    "    'Twitter/twhin-bert-large',\n",
    "    'facebook/xlm-v-base']\n",
    "\n",
    "#Cridem la funció per tots els models \n",
    "for model in llista_models:\n",
    "    masked_modeling(model, my_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segona funció reprodueix la mateixa anàlisi, però separa els resultats segons el tipus d’estructura gramatical: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_modeling_tipus(models, dataset, resum_resultats): \n",
    "  tokenizer_hf = AutoTokenizer.from_pretrained(models)\n",
    "  model = AutoModelForMaskedLM.from_pretrained(models)\n",
    "  if models == 'facebook/xmod-base':\n",
    "    model.set_default_language(\"ca_ES\")\n",
    "  pipeline = FillMaskPipeline(model, tokenizer_hf)\n",
    "\n",
    "  dataset = dataset.dropna() #per treure els valors buits, que si no, no puc per les prediccions\n",
    "  nom_fitxer = \"resultats_tipus.csv\"\n",
    "  fitxer_existeix = os.path.isfile(nom_fitxer)\n",
    "\n",
    "  for tipus in dataset[\"Tipus d'error\"].unique():\n",
    "    subset = dataset[dataset[\"Tipus d'error\"] == tipus].copy()\n",
    "\n",
    "  #Comptador per després\n",
    "    normatives_top5 = 0\n",
    "    normatives_top1 = 0\n",
    "    no_normatives_top5= 0\n",
    "    no_normatives_top1 = 0\n",
    "    altres = 0\n",
    "    prediccions_columna = [] #per afegir la columna de prediccions al dataset\n",
    "\n",
    "\n",
    "#Posa en una variable totes les frases amb <masked>, les respostes normatives i les no normatives; i fa la predicció per a cada frase amb <masked>\n",
    "\n",
    "    for index, row in subset.iterrows():\n",
    "      normatiu = row['Resposta normativa']\n",
    "      no_normatiu = row['Resposta no normativa freqüent']\n",
    "      masked = row[\"Masked\"]\n",
    "      res_hf = pipeline(masked)\n",
    "      prediccions =[r['token_str'] for r in res_hf]\n",
    "      prediccions_sense_espais = [pred.strip() for pred in prediccions] #fem strip perquè no surtin espais\n",
    "\n",
    "      if normatiu == prediccions_sense_espais[0]:\n",
    "        normatives_top1 +=1\n",
    "      if normatiu in prediccions_sense_espais:\n",
    "          normatives_top5 +=1\n",
    "      if no_normatiu==prediccions_sense_espais[0]:\n",
    "        no_normatives_top1 +=1\n",
    "      if no_normatiu in prediccions_sense_espais:\n",
    "        no_normatives_top5 +=1\n",
    "      if no_normatiu not in prediccions_sense_espais and normatiu not in prediccions_sense_espais:\n",
    "        altres +=1\n",
    "\n",
    "      prediccions_columna.append(prediccions_sense_espais)\n",
    "\n",
    "      #imprimim totes les prediccions, amb f string per imprimir variables que hem creat abans dins de la cadena\n",
    "\n",
    "    subset.to_csv(nom_fitxer, mode='a', index=False, header=not fitxer_existeix) #si existeix fa append del nou contingut\n",
    "    fitxer_existeix = True\n",
    "\n",
    "    #Afegim resultats per visualitzar les dades de forma més clara, ja que ara tenim més valors\n",
    "    resum_resultats.append({\n",
    "              \"Model\": models,\n",
    "              \"Tipus d'error\": tipus,\n",
    "              \"Normatives Top 1\": normatives_top1,\n",
    "              \"Normatives Top 5\": normatives_top5,\n",
    "              \"No normativess Top 1\": no_normatives_top1,\n",
    "              \"No normativess Top 5\": no_normatives_top5,\n",
    "              \"Altres\": altres\n",
    "      })\n",
    "\n",
    "\n",
    "resum_resultats = [] #fem una llista buida per l'append de dintre la funció\n",
    "\n",
    "for model in llista_models:\n",
    "  masked_modeling_tipus(model, my_dataset, resum_resultats)\n",
    "\n",
    "# Ho posem tot en un nou dataframe, per veure més fàcilment els resultats\n",
    "df_resum = pd.DataFrame(resum_resultats)\n",
    "df_resum.to_csv(\"resum_resultats.csv\", index=False)\n",
    "\n",
    "# Descarreguem els arxius\n",
    "files.download(\"resum_resultats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anàfores\n",
    "\n",
    "Per l’anàlisi de les anàfores s’ha desenvolupat una funció que, en una frase com “Va ser a la Maria a qui va alegrar la Laura quan va venir a la nostra ciutat; per tant, la `<mask>` va venir a la nostra ciutat.”, detecta si la primera predicció feta pel model coincideix amb el subjecte (Laura) o l’objecte directe (Maria). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anafora(models, dataset):\n",
    "  tokenizer_hf = AutoTokenizer.from_pretrained(models)\n",
    "  model = AutoModelForMaskedLM.from_pretrained(models)\n",
    "  if models == 'facebook/xmod-base':\n",
    "    model.set_default_language(\"ca_ES\")\n",
    "  pipeline = FillMaskPipeline(model, tokenizer_hf)\n",
    "\n",
    "  prediccions_columna = []\n",
    "\n",
    "#Comptador per després\n",
    "  numero_subj = 0\n",
    "  numero_od = 0\n",
    "\n",
    "  dataset = dataset.dropna()\n",
    "\n",
    "#Per cada línia, posa en una varaible l'objecte directe, el dubjecte, i les frases amb <mask>\n",
    "  for index, row in dataset.iterrows():\n",
    "    od = row['Objecte directe']\n",
    "    subj = row['Subjecte']\n",
    "    masked = row[\"Masked\"]\n",
    "\n",
    "#Realitza les prediccions\n",
    "    res_hf = pipeline(masked)\n",
    "    prediccions =[r['token_str'] for r in res_hf]\n",
    "    prediccions_sense_espais = [pred.strip() for pred in prediccions] #fem strip perquè no surtin espais\n",
    "    prediccions_columna.append(prediccions_sense_espais[0])\n",
    "\n",
    "#Afegeix al comptador quan el model prediu el subjecte i quan prediu l'objecte directes\n",
    "    if subj == prediccions_sense_espais[0]:\n",
    "      numero_subj +=1\n",
    "    if od==prediccions_sense_espais[0]:\n",
    "      numero_od +=1\n",
    "\n",
    "  dataset = dataset.copy() #Per evitar errors\n",
    "\n",
    "#Creem les noves columnes\n",
    "  dataset[\"Prediccions\"] = prediccions_columna\n",
    "  dataset[\"Model\"] = models\n",
    "  dataset[\"Prediccions subjecte\"] = None\n",
    "  dataset[\"Prediccions objecte directe\"] = None\n",
    "\n",
    "  #Assignem només a la primera fila\n",
    "  dataset.loc[dataset.index[0], \"Prediccions subjecte\"] = numero_subj\n",
    "  dataset.loc[dataset.index[0], \"Prediccions objecte directe\"] = numero_od\n",
    "\n",
    "  #Ho exportem\n",
    "  nom_fitxer = \"resultats_anafora.csv\"\n",
    "\n",
    "  fitxer_existeix = os.path.isfile(nom_fitxer) #mira si troba el fitxer als files\n",
    "  dataset.to_csv(nom_fitxer, mode='a', index=False, header=not fitxer_existeix) #si existeix fa append del nou contingut\n",
    "  files.download(nom_fitxer) #ho descarrega\n",
    "\n",
    "#Cridem la funció per tots els models\n",
    "for model in llista_models:\n",
    "  anafora(model, anafora_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igual que en el cas anterior, s’ha afegit una funció addicional que agrupa els resultats segons el tipus d’estructura de les oracions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funció anafora per tipus\n",
    "\n",
    "def anafora_tipus(models, dataset):\n",
    "  tokenizer_hf = AutoTokenizer.from_pretrained(models)\n",
    "  model = AutoModelForMaskedLM.from_pretrained(models)\n",
    "  if models == 'facebook/xmod-base':\n",
    "      model.set_default_language(\"ca_ES\")\n",
    "  pipeline = FillMaskPipeline(model=model, tokenizer=tokenizer_hf)\n",
    "\n",
    "  dataset = dataset.dropna()\n",
    "  resultats_totals = []\n",
    "\n",
    "  for tipus in dataset[\"Tipus\"].unique(): #per cada tipus d'estructura (valor únic en Tipus) creem un subset\n",
    "    subset = dataset[dataset[\"Tipus\"] == tipus].copy() #per evitar errors\n",
    "\n",
    "#Comptador per després\n",
    "    prediccions_columna = []\n",
    "    numero_subj = 0\n",
    "    numero_od = 0\n",
    "\n",
    "#Per cada línia de cada subset, posa en una varaible l'objecte directe, el dubjecte, i les frases amb <mask>\n",
    "    for index, row in subset.iterrows():\n",
    "      od = row['Objecte directe']\n",
    "      subj = row['Subjecte']\n",
    "      masked = row[\"Masked\"]\n",
    "\n",
    "#Realitza les prediccions\n",
    "      res_hf = pipeline(masked)\n",
    "      prediccions = [r['token_str'].strip() for r in res_hf]\n",
    "\n",
    "      prediccions_columna.append(prediccions[0])\n",
    "\n",
    "#Les afegeix al comptador\n",
    "      if subj == prediccions[0]:\n",
    "          numero_subj += 1\n",
    "      if od == prediccions[0]:\n",
    "          numero_od += 1\n",
    "\n",
    "#Creem les noves columnes\n",
    "    subset[\"Prediccions\"] = prediccions_columna\n",
    "    subset[\"Model\"] = models\n",
    "    subset[\"Tipus\"]= tipus\n",
    "    subset[\"Prediccions subjecte\"] = None\n",
    "    subset[\"Prediccions objecte directe\"] = None\n",
    "    subset.loc[subset.index[0], \"Prediccions subjecte\"] = numero_subj\n",
    "    subset.loc[subset.index[0], \"Prediccions objecte directe\"] = numero_od\n",
    "\n",
    "#Fem append perquè surtin els resultats de cada tipus d'estructura als resultats totals\n",
    "    resultats_totals.append(subset)\n",
    "\n",
    "\n",
    "  resultats_final = pd.concat(resultats_totals, ignore_index=True)\n",
    "  resultats_final = resultats_final.dropna(subset=[\"Prediccions subjecte\"])\n",
    "\n",
    "#Si existeix el fitxer també es fa append dels resultats per al model següent\n",
    "  nom_fitxer = \"resultats_anafora_tipus.csv\"\n",
    "  fitxer_existeix = os.path.isfile(nom_fitxer)\n",
    "  resultats_final.to_csv(nom_fitxer, mode='a', index=False, header=not fitxer_existeix)\n",
    "\n",
    "  files.download(nom_fitxer)\n",
    "\n",
    "#Cridem la funció per tots els models\n",
    "for model in llista_models:\n",
    "  anafora_tipus(model, anafora_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOM \n",
    "Per a les frases amb DOM, s’ha creat una funció que calcula la probabilitat logarítmica negativa de cada oració, tant amb com sense la preposició *a*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilitat_dom(models, dataset):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(models)\n",
    "  model = AutoModelForMaskedLM.from_pretrained(models)\n",
    "  if models == 'facebook/xmod-base':\n",
    "    model.set_default_language(\"ca_ES\")\n",
    "\n",
    "  #creem variables buides per afegir les prediccions i afegir les columnes més tard\n",
    "  preds_a = []\n",
    "  preds_no_a = []\n",
    "\n",
    "  #calculem les probabilitats, primer per les frases amb a\n",
    "  for sentence in dataset[\"Frase amb a\"]:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    preds_a.append(-outputs.loss.item())\n",
    "\n",
    "  #Després per les frases sense a\n",
    "  for sentence in dataset[\"Frase sense a \"]:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    preds_no_a.append(-outputs.loss.item())\n",
    "\n",
    "\n",
    "  #noves columnes\n",
    "  dataset[\"Prediccions a\"] = preds_a\n",
    "  dataset[\"Prediccions sense a\"] = preds_no_a\n",
    "  dataset[\"Model\"] = models\n",
    "\n",
    "\n",
    "  #Calculem les mitjanes\n",
    "\n",
    "  mean_a = dataset[\"Prediccions a\"].mean()\n",
    "  mean_sense_a = dataset[\"Prediccions sense a\"].mean()\n",
    "  mean_by_type = dataset.groupby(\"Tipus SN\")[[\"Prediccions a\", \"Prediccions sense a\"]].mean() #aquí calculem la mitjana segons el tipus d'estructura\n",
    "  std_a = dataset[\"Prediccions a\"].std()\n",
    "  std_sense_a = dataset[\"Prediccions sense a\"].std()\n",
    "\n",
    "#Buidem les columnes \n",
    "  dataset[\"Mitjana amb a\"]= None\n",
    "  dataset[\"Mitjana sense a\"]= None\n",
    "  dataset[\"Desviació estàndard amb a\"]= None\n",
    "  dataset[\"Desviació estàndard sense a\"]= None\n",
    "\n",
    "#Afegim només a la primera línia \n",
    "  dataset.loc[dataset.index[0], \"Mitjana amb a\"] = mean_a\n",
    "  dataset.loc[dataset.index[0], \"Mitjana sense a\"] = mean_sense_a\n",
    "  dataset.loc[dataset.index[0],\"Desviació estàndard amb a\"]= std_a\n",
    "  dataset.loc[dataset.index[0],\"Desviació estàndard sense a\"]= std_sense_a\n",
    "\n",
    "#Canviem el títol de les columnes\n",
    "  mean_by_type = mean_by_type.rename(columns={\n",
    "    \"Prediccions a\": \"Mitjana a tipus SN\",\n",
    "    \"Prediccions sense a\": \"Mitjana sense a tipus SN\"\n",
    "  })\n",
    "\n",
    "  dataset = dataset.merge(mean_by_type, on=\"Tipus SN\", how=\"left\")\n",
    "  dataset = dataset.drop(columns=[col for col in dataset.columns if \"Mitjanes\" in col])\n",
    "\n",
    "  #ho exportem\n",
    "  nom_fitxer = \"resultats_dom.csv\"\n",
    "\n",
    "  fitxer_existeix = os.path.isfile(nom_fitxer) #mira si troba el fitxer als files\n",
    "  dataset.to_csv(nom_fitxer, mode='a', index=False, header=not fitxer_existeix) #si existeix fa append del nou contingut\n",
    "  files.download(nom_fitxer) #ho descarrega\n",
    "\n",
    "#Cridem la funció per tots els models\n",
    "for model in llista_models:\n",
    "  probabilitat_dom(model, dom_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ser i estar \n",
    "\n",
    "Per a l’anàlisi de l’ús dels verbs ser i estar, s’ha implementat una funció anàloga que compara la probabilitat de les oracions amb el verb ser o el verb estar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilitat_ser_estar(models, dataset):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(models)\n",
    "  model = AutoModelForMaskedLM.from_pretrained(models)\n",
    "  if models == 'facebook/xmod-base':\n",
    "    model.set_default_language(\"ca_ES\")\n",
    "\n",
    "  #Creem variables buides per afegir les prediccions i afegir les columnes més tard\n",
    "  preds_ser = []\n",
    "  preds_estar = []\n",
    "\n",
    "  #Calculem les probabilitats, primer per les frases amb a\n",
    "  for sentence in dataset[\"Ser\"]:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    preds_ser.append(-outputs.loss.item())\n",
    "\n",
    "  #Després per les frases sense a\n",
    "  for sentence in dataset[\"Estar\"]:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    preds_estar.append(-outputs.loss.item())\n",
    "\n",
    "\n",
    "  #Noves columnes\n",
    "  dataset[\"Prediccions ser\"] = preds_ser\n",
    "  dataset[\"Prediccions estar\"] = preds_estar\n",
    "  dataset[\"Model\"] = models\n",
    "\n",
    "\n",
    "  #Calculem les mitjanes\n",
    "\n",
    "  mean_ser = dataset[\"Prediccions ser\"].mean()\n",
    "  mean_estar = dataset[\"Prediccions estar\"].mean()\n",
    "  std_ser = dataset[\"Prediccions ser\"].std()\n",
    "  std_estar = dataset[\"Prediccions estar\"].std()\n",
    "\n",
    "  dataset[\"Mitjana ser\"]= None\n",
    "  dataset[\"Mitjana estar\"]= None\n",
    "  dataset[\"Desviació estàndard ser\"]= None\n",
    "  dataset[\"Desviació estàndard estar\"]= None\n",
    "\n",
    "  dataset.loc[dataset.index[0], \"Mitjana ser\"] = mean_ser\n",
    "  dataset.loc[dataset.index[0], \"Mitjana estar\"] = mean_estar\n",
    "  dataset.loc[dataset.index[0],\"Desviació estàndard ser\"]= std_ser\n",
    "  dataset.loc[dataset.index[0],\"Desviació estàndard estar\"]= std_estar\n",
    "\n",
    "  #Ho exportem\n",
    "  nom_fitxer = \"resultats_ser_estar.csv\"\n",
    "\n",
    "  fitxer_existeix = os.path.isfile(nom_fitxer) #mira si troba el fitxer als files\n",
    "  dataset.to_csv(nom_fitxer, mode='a', index=False, header=not fitxer_existeix) #si existeix fa append del nou contingut\n",
    "  files.download(nom_fitxer) #ho descarrega\n",
    "\n",
    "#Cridem la funció per tots els models\n",
    "for model in llista_models:\n",
    "  probabilitat_ser_estar(model, ser_estar_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
